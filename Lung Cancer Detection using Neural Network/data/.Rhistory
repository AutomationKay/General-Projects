max_sp = TRUE,              # Maximum separation between nodes
alpha = 0.7,                # Transparency level for edges
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
x_names = c("GENDER", "AGE", "SMOKING", "YELLOW_FINGERS",
"ANXIETY", "PEER_PRESSURE", "CHRONIC_DISEASE",
"FATIGUE", "ALLERGY", "WHEEZING", "ALCOHOL_CONSUMING",
"COUGHING", "SHORTNESS_OF_BREATH",
"SWALLOWING_DIFFICULTY", "CHEST_PAIN"),
y_names = "LUNG_CANCER",
line_stag = 1)  # To adjust spacing between layers
# Define the formula for the neural network
formula <- LUNG_CANCER ~ GENDER + AGE + SMOKING + YELLOW_FINGERS + ANXIETY +
PEER_PRESSURE + `CHRONIC.DISEASE` + FATIGUE + ALLERGY + WHEEZING +
`ALCOHOL.CONSUMING` + COUGHING + `SHORTNESS.OF.BREATH` +
`SWALLOWING.DIFFICULTY` + `CHEST.PAIN`
# Train the neural network
set.seed(123)
nn <- neuralnet(formula, data=dataTrain, hidden=5, linear.output=FALSE)
# Plot the neural network
png("enhanced_neural_network_plot.png", width = 1600, height = 1200)
plotnet(nn,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 7,             # Increase the size of the nodes
node_labs = TRUE,           # Show node labels
cex_val = 0.8,              # Text size for labels
rel_rsc = 12,               # Control the relative scaling of edges
max_sp = TRUE,              # Maximum separation between nodes
alpha = 0.7,                # Transparency level for edges
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
x_names = c("GENDER", "AGE", "SMOKING", "YELLOW_FINGERS",
"ANXIETY", "PEER_PRESSURE", "CHRONIC_DISEASE",
"FATIGUE", "ALLERGY", "WHEEZING", "ALCOHOL_CONSUMING",
"COUGHING", "SHORTNESS_OF_BREATH",
"SWALLOWING_DIFFICULTY", "CHEST_PAIN"),
y_names = "LUNG_CANCER",
line_stag = 1)  # To adjust spacing between layers
dev.off()
# Define the formula for the neural network
formula <- LUNG_CANCER ~ GENDER + AGE + SMOKING + YELLOW_FINGERS + ANXIETY +
PEER_PRESSURE + `CHRONIC.DISEASE` + FATIGUE + ALLERGY + WHEEZING +
`ALCOHOL.CONSUMING` + COUGHING + `SHORTNESS.OF.BREATH` +
`SWALLOWING.DIFFICULTY` + `CHEST.PAIN`
# Train the neural network
set.seed(123)
nn <- neuralnet(formula, data=dataTrain, hidden=5, linear.output=FALSE)
# Plot the neural network
png("enhanced_neural_network_plot.png", width = 1600, height = 1200)
plotnet(nn,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 7,             # Increase the size of the nodes
node_labs = TRUE,           # Show node labels
cex_val = 0.8,              # Text size for labels
rel_rsc = 12,               # Control the relative scaling of edges
max_sp = TRUE,              # Maximum separation between nodes
alpha = 0.7,                # Transparency level for edges
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
x_names = c("GENDER", "AGE", "SMOKING", "YELLOW_FINGERS",
"ANXIETY", "PEER_PRESSURE", "CHRONIC_DISEASE",
"FATIGUE", "ALLERGY", "WHEEZING", "ALCOHOL_CONSUMING",
"COUGHING", "SHORTNESS_OF_BREATH",
"SWALLOWING_DIFFICULTY", "CHEST_PAIN"),
y_names = "LUNG_CANCER",
line_stag = 1)  # To adjust spacing between layers
dev.off()
# Define the formula for the neural network
formula <- LUNG_CANCER ~ GENDER + AGE + SMOKING + YELLOW_FINGERS + ANXIETY +
PEER_PRESSURE + `CHRONIC.DISEASE` + FATIGUE + ALLERGY + WHEEZING +
`ALCOHOL.CONSUMING` + COUGHING + `SHORTNESS.OF.BREATH` +
`SWALLOWING.DIFFICULTY` + `CHEST.PAIN`
# Train the neural network
set.seed(123)
nn <- neuralnet(formula, data=dataTrain, hidden=5, linear.output=FALSE)
# Plot the neural network
plotnet(nn,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 7,             # Increase the size of the nodes
node_labs = TRUE,           # Show node labels
cex_val = 0.8,              # Text size for labels
rel_rsc = 12,               # Control the relative scaling of edges
max_sp = TRUE,              # Maximum separation between nodes
alpha = 0.7,                # Transparency level for edges
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
x_names = c("GENDER", "AGE", "SMOKING", "YELLOW_FINGERS",
"ANXIETY", "PEER_PRESSURE", "CHRONIC_DISEASE",
"FATIGUE", "ALLERGY", "WHEEZING", "ALCOHOL_CONSUMING",
"COUGHING", "SHORTNESS_OF_BREATH",
"SWALLOWING_DIFFICULTY", "CHEST_PAIN"),
y_names = "LUNG_CANCER",
line_stag = 1)  # To adjust spacing between layers
# Define the formula for the neural network
formula <- LUNG_CANCER ~ GENDER + AGE + SMOKING + YELLOW_FINGERS + ANXIETY +
PEER_PRESSURE + `CHRONIC.DISEASE` + FATIGUE + ALLERGY + WHEEZING +
`ALCOHOL.CONSUMING` + COUGHING + `SHORTNESS.OF.BREATH` +
`SWALLOWING.DIFFICULTY` + `CHEST.PAIN`
# Train the neural network
set.seed(123)
nn <- neuralnet(formula, data=dataTrain, hidden=5, linear.output=FALSE)
# Plot the neural network
plotnet(nn,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 7,             # Increase the size of the nodes
node_labs = TRUE,           # Show node labels
cex_val = 0.8,              # Text size for labels
rel_rsc = 12,               # Control the relative scaling of edges
max_sp = TRUE,              # Maximum separation between nodes
alpha = 0.7,                # Transparency level for edges
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
x_names = c("GENDER", "AGE", "SMOKING", "YELLOW_FINGERS",
"ANXIETY", "PEER_PRESSURE", "CHRONIC_DISEASE",
"FATIGUE", "ALLERGY", "WHEEZING", "ALCOHOL_CONSUMING",
"COUGHING", "SHORTNESS_OF_BREATH",
"SWALLOWING_DIFFICULTY", "CHEST_PAIN"),
y_names = "LUNG_CANCER",
line_stag = 1)  # To adjust spacing between layers
# Define the model with Sigmoid activation and Binary Cross-Entropy loss
model_crossentropy <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = ncol(dataTrain) - 1) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model with Sigmoid activation and Binary Cross-Entropy loss
install_tensorflow()
install_tensorflow()
install.packages("tensorflow")
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(keras)
library(neuralnet)
library(caret)
library(pROC)
library(readr)  # For loading CSV files
library(dplyr)
library(NeuralNetTools)
library(tensorflow)
install_tensorflow()
install_tensorflow()
install_keras()
knitr::opts_chunk$set(echo = TRUE)
# Define the model with Sigmoid activation and Binary Cross-Entropy loss
model_crossentropy <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = ncol(dataTrain) - 1) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Load necessary libraries
library(keras)
library(neuralnet)
library(caret)
library(pROC)
library(readr)  # For loading CSV files
library(dplyr)
library(NeuralNetTools)
library(tensorflow)
library(reticulate)
# Define the model with Sigmoid activation and Binary Cross-Entropy loss
model_crossentropy <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model with Sigmoid activation and Binary Cross-Entropy loss
model_crossentropy <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model with Sigmoid activation and Binary Cross-Entropy loss
model_crossentropy <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
reticulate::py_last_error()
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# Compile the model
model %>% compile(
loss = 'binary_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
reticulate::py_last_error()
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
reticulate::py_last_error()
model <- keras_model_sequential()
model %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(ncol(dataTrain) - 1)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(nnet)
library(neuralnet)
library(caret)
library(pROC)
library(readr)  # For loading CSV files
library(NeuralNetTools)
library(reticulate)
library(magrittr)
library(ggplot2)
library(lattice)
library(smotefamily)
# Load the dataset
# https://www.kaggle.com/datasets/mysarahmadbhat/lung-cancer
data <- read_csv("survey_lung_cancer.csv")
# Displaying the first few rows of data
head(data)
# Convert categorical variables to numeric
data$GENDER <- ifelse(data$GENDER == "M", 1, 0)
data$LUNG_CANCER <- ifelse(data$LUNG_CANCER == "YES", 1, 0)
# Normalize numeric variables
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_normalized <- as.data.frame(lapply(data, normalize))
# Checking for missing values
sum(is.na(data_normalized))
# Display the first few rows of the preprocessed data
head(data_normalized)
# Setting a seed for reproducibility
set.seed(123)
# Split the data into training (70%) and testing (30%) sets
trainIndex <- createDataPartition(data_normalized$LUNG_CANCER, p = 0.7, list = FALSE, times = 1)
dataTrain <- data_normalized[ trainIndex,]
dataTest  <- data_normalized[-trainIndex,]
# Viewing summary statistic of the data set before proceeding
summary(dataTrain)
# Histogram of Age distribution
hist(data$AGE, main="Distribution of Age", xlab="Age", col="blue")
# Scatter plot: Age vs Smoking
plot(data$AGE, data$SMOKING, main="Age vs Smoking", xlab="Age", ylab="Smoking Level")
# Checking column names before training the neural network
names(dataTrain)
# Define the formula for the neural network
formula <- LUNG_CANCER ~ GENDER + AGE + SMOKING + YELLOW_FINGERS + ANXIETY +
PEER_PRESSURE + `CHRONIC.DISEASE` + FATIGUE + ALLERGY + WHEEZING +
`ALCOHOL.CONSUMING` + COUGHING + `SHORTNESS.OF.BREATH` +
`SWALLOWING.DIFFICULTY` + `CHEST.PAIN`
# Train the neural network
set.seed(123)
nn <- neuralnet(formula, data=dataTrain, hidden=5, linear.output=FALSE)
#summary of the model
summary(nn)
# Plot the neural network
png("neuralnetwork_plot.png", width = 2400, height = 1500, res = 200)
# Seting up margins to prevent labels from being cut off
par(mar = c(7, 7, 7, 7))  # bottom, left, top, right
plotnet(nn,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 6,             # Increase the size of the nodes
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
alpha = 0.6,                # Transparency level for edges
max_sp = TRUE,              # Maximum separation between nodes
rel_rsc = 15,               # Relative scaling of edges
cex = 0.4)                  # Reduce text size for labels
dev.off()
plotnet(nn,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 6,             # Increase the size of the nodes
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
alpha = 0.6,                # Transparency level for edges
max_sp = TRUE,              # Maximum separation between nodes
rel_rsc = 15,               # Relative scaling of edges
cex = 0.4)                  # Reduce text size for labels
# Exclude the LUNG_CANCER column from dataTest
input_data <- dataTest[, -ncol(dataTest)]
# Perform the computation using the neuralnet model
predictions_nn <- neuralnet::compute(nn, input_data)$net.result
# Convert to binary class predictions
predicted_class_nn <- ifelse(predictions_nn > 0.5, 1, 0)
# Create confusion matrix
confusion_matrix_nn <- table(predicted_class_nn, dataTest$LUNG_CANCER)
print(confusion_matrix_nn)
# Calculate accuracy
accuracy_nn <- sum(diag(confusion_matrix_nn)) / sum(confusion_matrix_nn)
print(paste("Accuracy with `neuralnet` model:", accuracy_nn))
# Plot ROC curve
roc_nn <- roc(dataTest$LUNG_CANCER, as.numeric(predicted_class_nn))
plot(roc_nn, main="ROC Curve - `neuralnet` Model")
print(paste("AUC with `neuralnet` model:", auc(roc_nn)))
# Training the model using 'tanh' activation function in 'neuralnet'
nn_tanh <- neuralnet(formula, data=dataTrain, hidden=10, act.fct = "tanh", linear.output = FALSE)
summary(nn_tanh)
# Using Mean Squared Error as loss function (in neuralnet)
nn_mse <- neuralnet(formula, data=dataTrain, hidden=10, act.fct = "logistic", err.fct = "sse", linear.output = FALSE)
summary(nn_mse)
# Convert target variable back to numeric (0 and 1)
y_train <- as.numeric(as.character(dataTrain$LUNG_CANCER))
# Removing the target variable 'LUNG_CANCER' from 'dataTrain'
x_train <- dataTrain[, -ncol(dataTrain)]
# Fit the neural network model for classification
nn_model <- nnet(x_train, y_train, size = 16, linout = FALSE, maxit = 200, decay = 0.001)
# Summary of the model
summary(nn_model)
# Save the improved plot as a high-resolution image
png("improved_nn_plot.png", width = 2400, height = 1500, res = 200)
# Setting up margins to prevent labels from being cut off
par(mar = c(5, 5, 5, 5))  # bottom, left, top, right
# Visualize the nnet model using plotnet
plotnet(nn_model,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 6,             # Increase the size of the nodes
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
alpha = 0.6,                # Transparency level for edges
max_sp = TRUE,              # Maximum separation between nodes
rel_rsc = 15,               # Relative scaling of edges
cex = 0.3)                  # Reduce text size for labels
dev.off()
# Visualize the nnet model using plotnet
plotnet(nn_model,
circle_col = "lightblue",   # Color of the nodes
circle_cex = 6,             # Increase the size of the nodes
pos_col = "blue",           # Color for positive weights
neg_col = "red",            # Color for negative weights
alpha = 0.6,                # Transparency level for edges
max_sp = TRUE,              # Maximum separation between nodes
rel_rsc = 15,               # Relative scaling of edges
cex = 0.3)                  # Reduce text size for labels
# Make predictions with type = "class"
predictions <- predict(nn_model, dataTest[, -ncol(dataTest)])
# Evaluate performance
confusion_matrix <- table(predictions, dataTest$LUNG_CANCER)
print(confusion_matrix)
# Calculate and display accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy with `nnet` model:", accuracy))
# Plot ROC curve
roc_nnet <- roc(dataTest$LUNG_CANCER, as.numeric(predictions))
plot(roc_nnet, main="ROC Curve - `nnet` Model")
print(paste("AUC with `nnet` model:", auc(roc_nnet)))
# Predictions with the MSE loss function
predictions_mse <- neuralnet::compute(nn_mse, dataTest[, -ncol(dataTest)])$net.result
predicted_class_mse <- ifelse(predictions_mse > 0.5, 1, 0)
# Create confusion matrix
confusion_matrix_mse <- table(predicted_class_mse, dataTest$LUNG_CANCER)
# Calculate and display accuracy
accuracy_mse <- sum(diag(confusion_matrix_mse)) / sum(confusion_matrix_mse)
print(paste("Accuracy with MSE loss function:", accuracy_mse))
smote_data <- SMOTE(dataTrain[, -ncol(dataTrain)], dataTrain$LUNG_CANCER, K=5, dup_size = 1)
balanced_dataTrain <- smote_data$data
balanced_dataTrain$LUNG_CANCER <- as.factor(balanced_dataTrain$class)
balanced_dataTrain$class <- NULL
table(balanced_dataTrain$LUNG_CANCER)
# Train the neural network
set.seed(123)
nn_balanced <- neuralnet(formula, data=balanced_dataTrain, hidden=5, linear.output=FALSE)
# Summary of the model
summary(nn_balanced)
nrow(dataTest[, -ncol(dataTest)])
predictions_balanced <- compute(nn_balanced, dataTest[, -ncol(dataTest)])$net.result
predicted_class_balanced <- ifelse(predictions_balanced > 0.5, 1, 0)
length(predicted_class_balanced)
length(dataTest$LUNG_CANCER)
dim(dataTest[, -ncol(dataTest)])
dim(as.data.frame(predictions_balanced))
head(predicted_class_balanced)
predictions_balanced <- compute(nn_balanced, dataTest[, -ncol(dataTest)])$net.result[, 2]
# Convert probabilities to binary class predictions
predicted_class_balanced <- ifelse(predictions_balanced > 0.5, 1, 0)
# Create confusion matrix
confusion_matrix_balanced <- table(predicted_class_balanced, dataTest$LUNG_CANCER)
print(confusion_matrix_balanced)
# Calculate and display accuracy
accuracy_balanced <- sum(diag(confusion_matrix_balanced)) / sum(confusion_matrix_balanced)
print(paste("Accuracy with SMOTE-balanced `nnet` model:", accuracy_balanced))
# Plot ROC curve for the SMOTE-balanced `nnet` model
roc_balanced <- roc(dataTest$LUNG_CANCER, as.numeric(predicted_class_balanced))
plot(roc_balanced, main="ROC Curve - SMOTE Balanced `nnet` Model")
# Calculate and print the AUC
auc_balanced <- auc(roc_balanced)
print(paste("AUC with SMOTE-balanced `nnet` model:", auc_balanced))
# Cross-validation setup
set.seed(123)
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE)  # 5-fold cross-validation
# Grid for hyperparameter tuning
tune_grid <- expand.grid(size = c(16, 32),  # Increase number of hidden neurons
decay = c(0.001, 0.01, 0.1))
# Train the neural network model with cross-validation
nn_model_cv <- train(formula, data = dataTrain, method = "nnet",
trControl = train_control,
tuneGrid = tune_grid,
linout = FALSE, maxit = 500)  # Increase maxit
# Summary of the model
summary(nn_model_cv)
# Make predictions on the test set
predictions <- predict(nn_model_cv, dataTest)
# Evaluate performance
confusion_matrix <- table(predictions, dataTest$LUNG_CANCER)
print(confusion_matrix)
# Calculate and display accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy with tuned `nnet` model:", accuracy))
# Plot ROC curve
roc_nnet_tuned <- roc(dataTest$LUNG_CANCER, as.numeric(predictions))
plot(roc_nnet_tuned, main="ROC Curve - Tuned `nnet` Model")
print(paste("AUC with tuned `nnet` model:", auc(roc_nnet)))
# Generate the ROC curve for the SMOTE-balanced nnet model
roc_smote_nnet <- roc(dataTest$LUNG_CANCER, as.numeric(predicted_class_balanced))
# Plot the ROC curve for the original nnet model
plot(roc_nnet, col = "red", main = "Comparison of ROC Curves", lwd = 2)
# Add the ROC curve for the SMOTE-balanced nnet model
lines(roc_smote_nnet, col = "blue", lwd = 2)
# Add the ROC curve for the tuned nnet model
lines(roc_nnet_tuned, col = "green", lwd = 2)
# Add a legend to the plot
legend("bottomright", legend = c("Original nnet", "SMOTE nnet", "Tuned nnet"),
col = c("red", "blue", "green"), lwd = 2)
# Accuracy values
accuracy_nnet <- 0.01
accuracy_nnet_smote <- 0.91
accuracy_nnet_tuned <- 0.02
# Create a data frame for the comparison
accuracy_comparison <- data.frame(
Model = c("Original nnet", "SMOTE nnet", "Tuned nnet"),
Accuracy = c(accuracy_nnet, accuracy_nnet_smote, accuracy_nnet_tuned)
)
# Print the comparison table
print(accuracy_comparison)
# Accuracy Comparison plot
barplot(accuracy_comparison$Accuracy, names.arg = accuracy_comparison$Model,
col = c("red", "blue", "green"), ylim = c(0, 1),
main = "Accuracy Comparison of Models",
ylab = "Accuracy")
# AUC values
auc_nnet <- 0.76
auc_nnet_smote <- 0.73
auc_nnet_tuned <- 0.93
# Create a data frame for the AUC comparison
auc_comparison <- data.frame(
Model = c("Original nnet", "SMOTE nnet", "Tuned nnet"),
AUC = c(auc_nnet, auc_nnet_smote, auc_nnet_tuned)
)
# Print the comparison table
print(auc_comparison)
# AUC Comparison plot
barplot(auc_comparison$AUC, names.arg = auc_comparison$Model,
col = c("red", "blue", "green"), ylim = c(0, 1),
main = "AUC Comparison of Models",
ylab = "AUC")
# For the best nnet model (tuned)
predictions_best_nnet <- predict(nn_model_cv, dataTest[, -ncol(dataTest)])
confusion_matrix_best_nnet <- table(predictions_best_nnet, dataTest$LUNG_CANCER)
accuracy_best_nnet <- sum(diag(confusion_matrix_best_nnet)) / sum(confusion_matrix_best_nnet)
# For the neuralnet model
predictions_nn <- neuralnet::compute(nn, input_data)$net.result
predicted_class_nn <- ifelse(predictions_nn > 0.5, 1, 0)
confusion_matrix_nn <- table(predicted_class_nn, dataTest$LUNG_CANCER)
accuracy_nn <- sum(diag(confusion_matrix_nn)) / sum(confusion_matrix_nn)
# Print accuracy
print(paste("Accuracy with best `nnet` model:", accuracy_best_nnet))
print(paste("Accuracy with `neuralnet` model:", accuracy_nn))
# Plot ROC Curves for both models
roc_best_nnet <- roc(dataTest$LUNG_CANCER, as.numeric(predictions_best_nnet))
roc_nn <- roc(dataTest$LUNG_CANCER, as.numeric(predicted_class_nn))
plot(roc_best_nnet, main="Comparison of ROC Curves: Best `nnet` vs `neuralnet`", col="red")
lines(roc_nn, col="blue")
legend("bottomright", legend=c("Best nnet", "neuralnet"), col=c("red", "blue"), lwd=2)
# Compare AUC values
auc_best_nnet <- auc(roc_best_nnet)
auc_nn <- auc(roc_nn)
barplot(c(auc_best_nnet, auc_nn), names.arg=c("Best nnet", "neuralnet"), col=c("red", "blue"), main="AUC Comparison")
